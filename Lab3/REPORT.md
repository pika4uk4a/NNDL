## Отчёт по лабораторной работе

Выполнила:

-------------|---------------------|------
Студент (ФИО) | Роль в проекте   | Оценка
-------------|---------------------|------
Крижановская Анна Александровна | Полная реализация проекта: сбор данных, все модели, анализ результатов | TBD

**Датасет:** главы фанфика *Гарри Поттер и методы рационального мышления* (HPMOR). Взяты первые 10 глав для обучения всех моделей.
**Формат токенизации:** посимвольная, по-словная, BPE.

---

##  Simple RNN

- **Токенизация:** посимвольная и по-словная
- **Эпох:** 200 и 50
- **Архитектура:** Однослойная RNN с 128 скрытыми единицами

###  Генерация (char-level):
```
Текст на основе слова 'язык':
языкая песпетенное падирмания овретевелшей честв. котово мести. гарри она дрого маназна нагка, которые резульнутидально, порудул дразое! пресмот, по пропятанее, истоваю был геру, он пытахотицы это на мести. арадом потождан, котерсиреттилась, когда сливатьсте стале вашусть задаматак, наменденто. амаю сто
```
```
Текст на основе слова 'зелье':
зельену, и они не заметиленный деть гонагичествое правиль я подаждай. я течее делсю, было не мый открыла оничиное которые мое что вы семься садарования, которыя наждимате моно булечтоб? я стелу подертув так мо сейчашиемстной наговорде фасфакцифинка оморшаю. роначасть если пробульствая термить помождывова
```

**Комментарий:**  
Посимвольная генерация показывает хорошее понимание примерного порядка согласных и гласных букв в словах, пока текст бессвязный.

---

## LSTM (однонаправленная, однослойная)

- **Токенизация:** посимвольно, по словам, BPE
- **Эпох:** 250, 100, 150
- **Архитектура:** LSTM с 256 скрытыми единицами

### Генерация (char-level):
```
Текст на основе слова 'гермиона':
гермиона урмух лискайком пер
```
```
Текст на основе слова 'пожиратели':
пожирателись мальнаго. гляна и

```

**Комментарий:**  
LSTM не показала значительных результатов, непонятно как раставлены знаки препинания.

---

## LSTM (многослойная)

- **Токенизация:** посимвольно, по словам, BPE
- **Эпох:** 50, 80, 100
- **Архитектура:** 2-слойная LSTM с 256 скрытыми единицами в каждом слое


### Генерация (word-level):
```
Текст на основе слова 'волшебная палочка':
волшебная палочка я не был вспомнить может быть развернулась ахава разговаривать интересно
```
```
Текст на основе слова 'малфой':
малфой и ни мальчик кашлянул же его запнулась на случай мой
```

### Генерация (BPE):
```
Текст на основе слова  'заклинание против':
заклинание против тёмного лорда , успокоил гарри . в магазине смертельное проклятие . на этом всё и кончилось . это проклятие формируется из чистой ненависти и бьёт прямо в душу , отделяя её сундук . к курицы ?.. ах ты задумался , сколько даже не знал со встроенной системой парня , встала
```
```
Текст на основе слова  'проклятые маглы':
прокля тые маглы , но что волшебники относились в свою помощницу , вы пожимаете им руки . в этом году он будет преподавать в магазине ингредиентов она старалась не видела , но не объясняйте , какое из них не раньше . гарри снова ! затем он ! да , сказал стоит часть за
```

**Комментарий:**  
Многослойность значительно улучшила качество генерации. Тексты стали более связными и содержательными, особенно с BPE токенизацией, но странная растановка знаков препинания.

---

##  Bidirectional LSTM

- **Эпох:** 50
- **Архитектура:** Двунаправленная LSTM с 128 скрытыми единицами

### Генерация (char-level):
```
Текст на основе слова 'заклинание':
заклинание магии оста. вольже. у вам давимастоврание. сейчас кти снаю
```

**Комментарий:**  
Bidirectional LSTM не спасает посимвольную генерацию, есть намеки на слова, но результат оставляет желать лучшего.

---

## GPT-style Transformer

- **Токенизация:** посимвольно и по словам
- **Эпох:** 60-80
- **Архитектура:** 4-слойный Transformer с 8 головками внимания

### Генерация (char-level):
```
Текст на основе слова 'гарри':
гарри с этинируев их гермионё чекала макгонагалл бынеи под серьёзно хорошо её пол, что одни, которой из, мистер поттер! ославне, и поднас, мистер результет. на людей шутки, потому, ведь ты чуть случайной. уюдишь, его снавидают широко о вама секрешите мог мартиах поттер! гарри что?!е поттер! ум? наконец де
```
```
Текст на основе слова 'гарри' (короткая генерация):
гарри мысего она родить говориру! называет ту,
```

**Комментарий:**  
Transformer показывает неплохие результаты, создавая длинные и местами связные фразы.

---

## Дообучение предобученной GPT (ruGPT3Small)

- **Модель:** `sberbank-ai/rugpt3small_based_on_gpt2`
- **Токенизация:** встроенная
- **Эпох:** 3

### Генерация:
```
Текст на основе фразы 'в хогвартсе':
в хогвартсе и Древняя магия не было. Я бы хотел, чтобы кто-то объяснил этот феномен. Может быть, я и не понимаю его, но, думаю, понимаю. Я хочу, чтобы всё в моей жизни наладилась.
```
```
Текст на основе фразы 'гермиона сказала'
гермиона сказала: «Вы должны были понять это раньше, в самом начале». Но не было ни секунды, чтобы понять это. И вам было просто стыдно за то, что вы делаете такую глупость. Мы должны дать объяснение, что привело вас к смерти!

Брайен! Брайан! Ты должен это видеть! Тебе нужно научиться говорить правду! Ты должен знать это! Брайан! Ты не должен был умирать! Не должен умирать, и ты должен был это выяснить и признать! И хотя ты очень молод, у тебя было не просто быть молодым, ты должен был стать взрослым! Ты должен был иметь возможность жить! И чтобы найти ответы, нужны были не только знания об этом, ты должен был научиться слушать собеседника! Ты должен был научиться использовать время, которое у тебя есть, в мирных целях, чтобы выжить, ты должен научиться выживать во все больше и больше опасностях, а потом вернуться, не сказав об этом больше!

Взбесившиеся псы начали затихать. Они не хотели, чтобы они понимали это! Но, кажется, вы поняли то, что я сказала, доктор?! Я хочу знать, почему ты был так категоричен в моей просьбе!

Брайен улыбнулся. Ему хотелось, чтобы псы понимали это раньше, в самом начале. «Я знал
```

**Комментарий:**  
Дообученная GPT показывает наилучшие результаты. Генерируемый текст максимально близок к стилю оригинала, сохраняя характерные особенности произведения.

---

## Сравнительный анализ моделей

| Модель                | Токенизация        | Качество текста | Связность | Пунктуация | Итоговая оценка |
|------------------------|--------------------|-----------------|-----------|------------|-----------------|
| Simple RNN             | char, word         | Бессвязный текст, случайные слова | Очень низкая | Удовлетворительная | 2/10 |
| LSTM (однослойная)     | char, word, BPE    | Короткие, примитивные фразы | Низкая | Случайная, хаотичная | 3/10 |
| LSTM (многослойная)    | char, word, BPE    | Более содержательный текст, лучше при BPE | Средняя | Ошибки, но читаемо | 5/10 |
| Bidirectional LSTM     | char-level         | Фрагменты слов, улучшение минимальное | Низкая | Случайная | 4/10 |
| Transformer            | char, word         | Длинные и относительно связные тексты | Высокая | Ошибки редки | 7/10 |
| ruGPT3Small (дообуч.)  | встроенная         | Стилистически близкие и осмысленные тексты | Очень высокая | Почти корректная | 9/10 |

## Выводы

1. **Simple RNN**  
   - Посимвольная и по-словная генерация.  
   - Текст несвязный, наблюдается лишь имитация структуры слов.  
   - Практическая ценность ограничена демонстрацией базового принципа работы рекуррентных сетей.  

2. **LSTM (однослойная)**  
   - Генерация короткая и фрагментарная.  
   - Структура предложений отсутствует, пунктуация используется случайным образом.  
   - Существенного улучшения по сравнению с RNN не наблюдается.  

3. **LSTM (многослойная)**  
   - Двухслойная архитектура демонстрирует более содержательные тексты.  
   - Использование BPE-токенизации значительно повышает связность.  
   - Недостатки: синтаксические ошибки и неправильная расстановка знаков препинания.  

4. **Bidirectional LSTM**  
   - Двунаправленная обработка контекста не решает проблемы несвязности при посимвольной генерации.  
   - Результат ограничивается обрывками слов и предложений.  

5. **Transformer**  
   - Генерация более связная и длинная, чем у LSTM.  
   - Контекст частично сохраняется, но встречаются бессмысленные обороты.  
   - Превосходит рекуррентные модели по качеству.  

6. **ruGPT3Small (дообученная)**  
   - Демонстрирует наилучшее качество генерации.  
   - Тексты стилистически близки к оригиналу, содержат осмысленные и структурированные фразы.  
   - Обеспечивает наибольшую практическую ценность среди всех рассмотренных моделей.  

---

### Общие выводы

- Наиболее эффективная **токенизация** — BPE, обеспечивающая связность и грамматическую корректность текста.  
- Увеличение глубины архитектуры и двунаправленность повышают качество, но не устраняют фундаментальные ограничения рекуррентных моделей.  
- **Transformer** значительно превосходит RNN-архитектуры по качеству генерации.  
- **Предобученные GPT-модели** обеспечивают результаты, максимально приближённые к реальным текстам.  

---

### Рекомендации

- Для генерации текста высокого качества целесообразно использовать **предобученные GPT-модели с дообучением**.  
- Для балансировки качества и вычислительных ресурсов возможно применение **Transformer** или **Bidirectional LSTM**.  
- Для учебных целей и понимания принципов работы рекуррентных архитектур можно использовать **Simple RNN** и однослойные **LSTM**.  

---
