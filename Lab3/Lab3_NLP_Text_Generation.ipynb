{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Генерация текста на основе \"Гарри Поттер и методы рационального мышления\"\n",
        "\n",
        "## Описание проекта\n",
        "\n",
        "Данный проект реализует различные подходы к **генерации текста** с использованием современных методов обработки естественного языка. В качестве обучающих данных используется фанфик \"Гарри Поттер и методы рационального мышления\" - произведение, сочетающее магический мир с научным подходом к мышлению.\n",
        "\n",
        "### Основные задачи:\n",
        "- **Сбор и предобработка данных**: Извлечение текста с веб-сайта и очистка\n",
        "- **Различные типы токенизации**: Посимвольная, пословная, BPE\n",
        "- **Архитектуры нейронных сетей**: RNN, LSTM, Bidirectional LSTM, Transformer\n",
        "- **Генерация текста**: Создание новых текстов в стиле оригинала\n",
        "\n",
        "### Изучаемые модели:\n",
        "1. **Simple RNN** - базовая рекуррентная сеть\n",
        "2. **LSTM** - долгосрочная память\n",
        "3. **Bidirectional LSTM** - двунаправленная обработка\n",
        "4. **GPT-style Transformer** - архитектура внимания\n",
        "\n",
        "### Начнем с загрузки и предобработки данных:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Сбор данных с веб-сайта\n",
        "\n",
        "### Загрузка текста из интернета\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Импорт необходимых библиотек для веб-скрапинга\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "# Базовый URL для загрузки глав\n",
        "base_url = \"https://hpmor.ru/book/1/\"\n",
        "all_text = \"\"\n",
        "\n",
        "print(\"Начинаем загрузку текста из интернета...\")\n",
        "print(\"Источник: https://hpmor.ru/book/1/\")\n",
        "\n",
        "# Загружаем первые 10 глав для демонстрации\n",
        "for chapter in tqdm(range(1, 11), desc=\"Загрузка глав\"):\n",
        "    url = f\"{base_url}{chapter}/\"\n",
        "    try:\n",
        "        # Отправляем GET-запрос\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Проверяем успешность запроса\n",
        "\n",
        "        # Парсим HTML-содержимое\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "        paragraphs = soup.find_all(\"p\")\n",
        "\n",
        "        # Ищем индекс абзаца со звездочками (разделитель между метаданными и текстом)\n",
        "        start_index = next(\n",
        "            (i for i, p in enumerate(paragraphs) if p.get_text(strip=True) == \"* * *\"),\n",
        "            None\n",
        "        )\n",
        "\n",
        "        if start_index is None:\n",
        "            print(f\"Глава {chapter}: звездочки не найдены, пропускаем\")\n",
        "            continue\n",
        "\n",
        "        # Берем только абзацы после звездочек и исключаем теги <em>\n",
        "        filtered_paragraphs = [\n",
        "            p.get_text(strip=True) for p in paragraphs[start_index + 1:]\n",
        "            if not p.find(\"em\")  # Исключаем курсивные примечания\n",
        "        ]\n",
        "\n",
        "        chapter_text = \"\\n\".join(filtered_paragraphs)\n",
        "\n",
        "        # Добавляем заголовок главы и текст\n",
        "        all_text += f\"\\n\\n=== Глава {chapter} ===\\n\\n\"\n",
        "        all_text += chapter_text\n",
        "\n",
        "        # Вежливая задержка между запросами\n",
        "        time.sleep(0.2)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Глава {chapter}: ошибка — {e}\")\n",
        "\n",
        "# Сохраняем результат в файл\n",
        "with open(\"harry.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(all_text)\n",
        "\n",
        "print(\"Загрузка завершена! Текст сохранен в файл: harry.txt\")\n",
        "print(f\"Общий размер текста: {len(all_text)} символов\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Предобработка текста\n",
        "\n",
        "### Функции очистки и нормализации\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Импорт библиотек для обработки текста\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "def remove_dialog_dashes(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Удаляет тире в начале строк (реплики диалога)\n",
        "    \n",
        "    Args:\n",
        "        text (str): Исходный текст\n",
        "        \n",
        "    Returns:\n",
        "        str: Текст без тире в начале строк\n",
        "    \"\"\"\n",
        "    # Удаляем тире, дефисы и длинные тире в начале строк\n",
        "    return re.sub(r\"(?m)^[\\s]*[–—-]\\s*\", \"\", text)\n",
        "\n",
        "def keep_only_russian(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Оставляет только русские буквы и базовые знаки препинания\n",
        "    \n",
        "    Args:\n",
        "        text (str): Исходный текст\n",
        "        \n",
        "    Returns:\n",
        "        str: Очищенный текст в нижнем регистре\n",
        "    \"\"\"\n",
        "    # Приводим к нижнему регистру\n",
        "    text = text.lower()\n",
        "    \n",
        "    # Оставляем только русские буквы, пробелы и основные знаки препинания\n",
        "    text = re.sub(r\"[^а-яё .,!?;\\n]\", \" \", text)\n",
        "    \n",
        "    # Убираем лишние пробелы\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    \n",
        "    return text.strip()\n",
        "\n",
        "print(\"Функции предобработки текста определены:\")\n",
        "print(\"- remove_dialog_dashes(): удаление тире из диалогов\")\n",
        "print(\"- keep_only_russian(): нормализация русского текста\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Загружаем сохраненный текст\n",
        "with open(\"harry.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    all_text = f.read()\n",
        "\n",
        "print(f\"Загружен текст размером: {len(all_text)} символов\")\n",
        "\n",
        "# Разделяем главы по заголовкам\n",
        "chapters_raw = re.split(r\"\\n+=+ Глава (\\d+) =+\\n+\", all_text)\n",
        "\n",
        "chapters = []\n",
        "for i in range(1, len(chapters_raw), 2):\n",
        "    chapter_number = int(chapters_raw[i])\n",
        "    chapter_text = chapters_raw[i + 1].strip()\n",
        "\n",
        "    # Применяем функции очистки\n",
        "    chapter_text = remove_dialog_dashes(chapter_text)\n",
        "    chapter_text = keep_only_russian(chapter_text)\n",
        "\n",
        "    chapters.append({\"chapter\": chapter_number, \"text\": chapter_text})\n",
        "\n",
        "# Создаем DataFrame для удобной работы с данными\n",
        "df = pd.DataFrame(chapters)\n",
        "df = df.sort_values(\"chapter\").reset_index(drop=True)\n",
        "\n",
        "print(f\"Обработано глав: {len(df)}\")\n",
        "print(\"\\nПримеры обработанных глав:\")\n",
        "print(df.head())\n",
        "\n",
        "# Объединяем весь очищенный текст\n",
        "full_cleaned_text = \" \".join(df[\"text\"].astype(str).tolist())\n",
        "print(f\"\\nОбщий размер очищенного текста: {len(full_cleaned_text)} символов\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Часть 1: Simple RNN\n",
        "\n",
        "## Посимвольная токенизация\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Импорт библиотек для глубокого обучения\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense\n",
        "\n",
        "print(\"Настраиваем Simple RNN для посимвольной генерации текста...\")\n",
        "\n",
        "# 1. Подготовка алфавита\n",
        "chars = sorted(set(full_cleaned_text))\n",
        "char2idx = {c: i for i, c in enumerate(chars)}\n",
        "idx2char = {i: c for c, i in char2idx.items()}\n",
        "vocab_size = len(chars)\n",
        "\n",
        "print(f\"Размер словаря символов: {vocab_size}\")\n",
        "print(f\"Первые 20 символов: {chars[:20]}\")\n",
        "\n",
        "# 2. Создание обучающих данных\n",
        "seq_length = 40  # Длина входной последовательности\n",
        "step = 3         # Шаг скользящего окна\n",
        "input_seqs = []\n",
        "target_chars = []\n",
        "\n",
        "print(\"Создаем обучающие последовательности...\")\n",
        "\n",
        "for i in range(0, len(full_cleaned_text) - seq_length, step):\n",
        "    input_seq = full_cleaned_text[i:i + seq_length]\n",
        "    target_char = full_cleaned_text[i + seq_length]\n",
        "    input_seqs.append([char2idx[c] for c in input_seq])\n",
        "    target_chars.append(char2idx[target_char])\n",
        "\n",
        "print(f\"Создано обучающих последовательностей: {len(input_seqs)}\")\n",
        "\n",
        "# 3. One-hot encoding\n",
        "X = tf.keras.utils.to_categorical(input_seqs, num_classes=vocab_size)\n",
        "y = tf.keras.utils.to_categorical(target_chars, num_classes=vocab_size)\n",
        "\n",
        "print(f\"Размер входных данных: {X.shape}\")\n",
        "print(f\"Размер целевых данных: {y.shape}\")\n",
        "\n",
        "# 4. Создание модели Simple RNN\n",
        "model = Sequential([\n",
        "    SimpleRNN(128, input_shape=(seq_length, vocab_size)),\n",
        "    Dense(vocab_size, activation='softmax')\n",
        "])\n",
        "\n",
        "print(\"Архитектура модели:\")\n",
        "model.summary()\n",
        "\n",
        "# Компиляция модели\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "print(\"Модель скомпилирована и готова к обучению!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Обучение модели Simple RNN\n",
        "print(\"Начинаем обучение Simple RNN...\")\n",
        "print(\"Это может занять некоторое время...\")\n",
        "\n",
        "# Обучение модели\n",
        "model.fit(X, y, batch_size=128, epochs=200)\n",
        "\n",
        "print(\"Обучение завершено!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_text(seed_text, gen_length=300):\n",
        "    \"\"\"\n",
        "    Функция для генерации текста с помощью обученной модели\n",
        "    \n",
        "    Args:\n",
        "        seed_text (str): Начальный текст для генерации\n",
        "        gen_length (int): Длина генерируемого текста\n",
        "        \n",
        "    Returns:\n",
        "        str: Сгенерированный текст\n",
        "    \"\"\"\n",
        "    generated = seed_text\n",
        "    \n",
        "    for _ in range(gen_length):\n",
        "        # Подготавливаем входную последовательность\n",
        "        input_seq = [char2idx.get(c, 0) for c in generated[-seq_length:]]\n",
        "        input_seq = tf.keras.utils.to_categorical(input_seq, num_classes=vocab_size)\n",
        "        input_seq = np.expand_dims(input_seq, axis=0)  # Добавляем размерность батча\n",
        "\n",
        "        # Получаем предсказание следующего символа\n",
        "        prediction = model.predict(input_seq, verbose=0)[0]\n",
        "        \n",
        "        # Выбираем следующий символ на основе вероятностей\n",
        "        next_index = np.random.choice(range(vocab_size), p=prediction)\n",
        "        next_char = idx2char[next_index]\n",
        "\n",
        "        generated += next_char\n",
        "    \n",
        "    return generated\n",
        "\n",
        "# Тестируем генерацию текста\n",
        "print(\"Генерируем текст на основе слова 'язык':\")\n",
        "generated_text1 = generate_text(\"язык\")\n",
        "print(generated_text1)\n",
        "\n",
        "print(\"\\nГенерируем текст на основе слова 'зелье':\")\n",
        "generated_text2 = generate_text(\"зелье\")\n",
        "print(generated_text2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Пословная токенизация\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Настройка Simple RNN для пословной генерации\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "print(\"Настраиваем Simple RNN для пословной генерации...\")\n",
        "\n",
        "# 1. Подготовка: токенизация текста на слова\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([full_cleaned_text])\n",
        "\n",
        "word_index = tokenizer.word_index          # Слово → индекс\n",
        "index_word = {v: k for k, v in word_index.items()}  # Индекс → слово\n",
        "total_words = len(word_index) + 1          # Добавляем 1 для padding\n",
        "\n",
        "print(f\"Всего уникальных слов: {total_words}\")\n",
        "print(f\"Первые 10 слов в словаре: {list(word_index.keys())[:10]}\")\n",
        "\n",
        "# 2. Создание обучающих последовательностей (n-граммы)\n",
        "input_sequences = []\n",
        "words = full_cleaned_text.split()\n",
        "\n",
        "# Скользящее окно для генерации n-грамм\n",
        "window_size = 5\n",
        "for i in range(1, len(words)):\n",
        "    seq = words[max(0, i - window_size):i + 1]\n",
        "    encoded = tokenizer.texts_to_sequences([\" \".join(seq)])[0]\n",
        "    if len(encoded) >= 2:\n",
        "        input_sequences.append(encoded)\n",
        "\n",
        "print(f\"Количество обучающих последовательностей: {len(input_sequences)}\")\n",
        "\n",
        "# 3. Подготовка X и y\n",
        "max_seq_len = max(len(seq) for seq in input_sequences)\n",
        "input_sequences = pad_sequences(input_sequences, maxlen=max_seq_len)\n",
        "\n",
        "X_words = input_sequences[:, :-1]  # все слова, кроме последнего\n",
        "y_words = tf.keras.utils.to_categorical(input_sequences[:, -1], num_classes=total_words)\n",
        "\n",
        "print(f\"Размер входных данных: {X_words.shape}\")\n",
        "print(f\"Размер целевых данных: {y_words.shape}\")\n",
        "\n",
        "# 4. Построение модели Simple RNN для слов\n",
        "model_word = Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=total_words, output_dim=64, input_length=X_words.shape[1]),\n",
        "    SimpleRNN(128),\n",
        "    Dense(total_words, activation='softmax')\n",
        "])\n",
        "\n",
        "model_word.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "print(\"Архитектура модели для слов:\")\n",
        "model_word.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Часть 2: LSTM (Long Short-Term Memory)\n",
        "\n",
        "## Однонаправленная LSTM\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Посимвольная токенизация с LSTM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Импорт библиотек для LSTM\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "print(\"Настраиваем LSTM для посимвольной генерации...\")\n",
        "\n",
        "# Подготовка данных (аналогично Simple RNN)\n",
        "chars = sorted(set(full_cleaned_text))\n",
        "char2idx = {c: i for i, c in enumerate(chars)}\n",
        "idx2char = {i: c for c, i in char2idx.items()}\n",
        "vocab_size = len(chars)\n",
        "\n",
        "print(f\"Размер словаря символов: {vocab_size}\")\n",
        "\n",
        "# Создание последовательностей\n",
        "seq_length = 40\n",
        "step = 3\n",
        "sequences = []\n",
        "next_chars = []\n",
        "\n",
        "for i in range(0, len(full_cleaned_text) - seq_length, step):\n",
        "    seq = full_cleaned_text[i:i+seq_length]\n",
        "    target = full_cleaned_text[i+seq_length]\n",
        "    sequences.append([char2idx[c] for c in seq])\n",
        "    next_chars.append(char2idx[target])\n",
        "\n",
        "# Преобразование в one-hot encoding\n",
        "X = to_categorical(sequences, num_classes=vocab_size)\n",
        "y = to_categorical(next_chars, num_classes=vocab_size)\n",
        "\n",
        "print(f\"Размер обучающих данных: {X.shape}\")\n",
        "print(f\"Размер целевых данных: {y.shape}\")\n",
        "\n",
        "# Создание модели LSTM\n",
        "model_lstm_1 = Sequential([\n",
        "    LSTM(128, input_shape=(seq_length, vocab_size)),\n",
        "    Dense(vocab_size, activation='softmax')\n",
        "])\n",
        "\n",
        "model_lstm_1.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "print(\"Архитектура однослойной LSTM:\")\n",
        "model_lstm_1.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Обучение однослойной LSTM\n",
        "print(\"Начинаем обучение однослойной LSTM...\")\n",
        "\n",
        "model_lstm_1.fit(X, y, batch_size=128, epochs=50)\n",
        "\n",
        "print(\"Обучение однослойной LSTM завершено!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_text_char_lstm(model, seed_text=\"гарри\", length=20, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Функция генерации текста с использованием LSTM\n",
        "    \n",
        "    Args:\n",
        "        model: Обученная LSTM модель\n",
        "        seed_text (str): Начальный текст\n",
        "        length (int): Длина генерируемого текста\n",
        "        temperature (float): Температура для сэмплирования\n",
        "        \n",
        "    Returns:\n",
        "        str: Сгенерированный текст\n",
        "    \"\"\"\n",
        "    generated = seed_text\n",
        "    \n",
        "    for _ in range(length):\n",
        "        # Подготавливаем входную последовательность\n",
        "        input_seq = [char2idx.get(c, 0) for c in generated[-seq_length:]]\n",
        "        input_seq = tf.keras.utils.to_categorical(input_seq, num_classes=vocab_size)\n",
        "        input_seq = np.expand_dims(input_seq, axis=0)\n",
        "\n",
        "        # Получаем предсказания\n",
        "        preds = model.predict(input_seq, verbose=0)[0]\n",
        "        preds = np.asarray(preds).astype(\"float64\")\n",
        "        \n",
        "        # Применяем температуру для контроля случайности\n",
        "        preds = np.log(preds + 1e-8) / temperature\n",
        "        exp_preds = np.exp(preds)\n",
        "        preds = exp_preds / np.sum(exp_preds)\n",
        "\n",
        "        # Выбираем следующий символ\n",
        "        next_idx = np.random.choice(range(vocab_size), p=preds)\n",
        "        next_char = idx2char[next_idx]\n",
        "        generated += next_char\n",
        "    \n",
        "    return generated\n",
        "\n",
        "# Тестируем генерацию с однослойной LSTM\n",
        "print(\"Генерация текста с однослойной LSTM:\")\n",
        "print(\"Начальное слово: 'гермиона'\")\n",
        "generated_text = generate_text_char_lstm(model_lstm_1, \"гермиона\")\n",
        "print(generated_text)\n",
        "\n",
        "print(\"\\nГенерация с другим начальным словом:\")\n",
        "print(\"Начальное слово: 'пожиратели'\")\n",
        "generated_text2 = generate_text_char_lstm(model_lstm_1, \"пожиратели\")\n",
        "print(generated_text2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Многослойная LSTM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Создание многослойной LSTM модели\n",
        "print(\"Создаем многослойную LSTM модель...\")\n",
        "\n",
        "model_lstm_multi = Sequential([\n",
        "    LSTM(128, return_sequences=True, input_shape=(seq_length, vocab_size)),\n",
        "    LSTM(128),\n",
        "    Dense(vocab_size, activation='softmax')\n",
        "])\n",
        "\n",
        "model_lstm_multi.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "print(\"Архитектура многослойной LSTM:\")\n",
        "model_lstm_multi.summary()\n",
        "\n",
        "# Обучение многослойной LSTM\n",
        "print(\"Начинаем обучение многослойной LSTM...\")\n",
        "model_lstm_multi.fit(X, y, batch_size=128, epochs=50)\n",
        "\n",
        "print(\"Обучение многослойной LSTM завершено!\")\n",
        "\n",
        "# Тестируем многослойную LSTM\n",
        "print(\"\\nГенерация текста с многослойной LSTM:\")\n",
        "print(\"Начальное слово: 'драко'\")\n",
        "generated_multi = generate_text_char_lstm(model_lstm_multi, \"драко\")\n",
        "print(generated_multi)\n",
        "\n",
        "print(\"\\nГенерация с другим начальным словом:\")\n",
        "print(\"Начальное слово: 'поттер'\")\n",
        "generated_multi2 = generate_text_char_lstm(model_lstm_multi, \"поттер\")\n",
        "print(generated_multi2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Пословная токенизация с LSTM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Настройка LSTM для пословной генерации\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "print(\"Настраиваем LSTM для пословной генерации...\")\n",
        "\n",
        "# Токенизация слов\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([full_cleaned_text])\n",
        "word_index = tokenizer.word_index\n",
        "index_word = {v: k for k, v in word_index.items()}\n",
        "total_words = len(word_index) + 1\n",
        "\n",
        "print(f\"Общий словарь слов: {total_words}\")\n",
        "\n",
        "# Создание последовательностей слов\n",
        "input_sequences = []\n",
        "words = full_cleaned_text.split()\n",
        "\n",
        "for i in range(1, len(words)):\n",
        "    ngram = words[max(0, i-5):i+1]\n",
        "    encoded = tokenizer.texts_to_sequences([\" \".join(ngram)])[0]\n",
        "    if len(encoded) >= 2:\n",
        "        input_sequences.append(encoded)\n",
        "\n",
        "max_len = max(len(seq) for seq in input_sequences)\n",
        "input_sequences = pad_sequences(input_sequences, maxlen=max_len)\n",
        "\n",
        "X_words = input_sequences[:, :-1]\n",
        "y_words = to_categorical(input_sequences[:, -1], num_classes=total_words)\n",
        "\n",
        "print(f\"Размер обучающих данных: {X_words.shape}\")\n",
        "print(f\"Размер целевых данных: {y_words.shape}\")\n",
        "\n",
        "# Создание однослойной LSTM для слов\n",
        "model_words_1 = Sequential([\n",
        "    tf.keras.layers.Embedding(total_words, 64, input_length=X_words.shape[1]),\n",
        "    LSTM(128),\n",
        "    Dense(total_words, activation='softmax')\n",
        "])\n",
        "\n",
        "model_words_1.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "print(\"Архитектура однослойной LSTM для слов:\")\n",
        "model_words_1.summary()\n",
        "\n",
        "# Обучение модели\n",
        "print(\"Начинаем обучение однослойной LSTM для слов...\")\n",
        "model_words_1.fit(X_words, y_words, epochs=100, batch_size=128)\n",
        "\n",
        "print(\"Обучение завершено!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_text_word_lstm(model, seed_text, num_words=10, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Функция генерации текста по словам с использованием LSTM\n",
        "    \n",
        "    Args:\n",
        "        model: Обученная LSTM модель\n",
        "        seed_text (str): Начальный текст\n",
        "        num_words (int): Количество слов для генерации\n",
        "        temperature (float): Температура для сэмплирования\n",
        "        \n",
        "    Returns:\n",
        "        str: Сгенерированный текст\n",
        "    \"\"\"\n",
        "    for _ in range(num_words):\n",
        "        # Токенизируем текущий текст\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=X_words.shape[1])\n",
        "\n",
        "        # Получаем предсказания\n",
        "        preds = model.predict(token_list, verbose=0)[0]\n",
        "        preds = np.log(preds + 1e-8) / temperature\n",
        "        exp_preds = np.exp(preds)\n",
        "        preds = exp_preds / np.sum(exp_preds)\n",
        "\n",
        "        # Выбираем следующее слово\n",
        "        next_index = np.random.choice(range(total_words), p=preds)\n",
        "        next_word = index_word.get(next_index, \"\")\n",
        "        seed_text += \" \" + next_word\n",
        "    \n",
        "    return seed_text\n",
        "\n",
        "# Тестируем генерацию слов\n",
        "print(\"Генерация текста по словам с LSTM:\")\n",
        "print(\"Начальная фраза: 'волшебная палочка'\")\n",
        "generated_words = generate_text_word_lstm(model_words_1, \"волшебная палочка\")\n",
        "print(generated_words)\n",
        "\n",
        "print(\"\\nГенерация с другой фразой:\")\n",
        "print(\"Начальная фраза: 'малфой'\")\n",
        "generated_words2 = generate_text_word_lstm(model_words_1, \"малфой\")\n",
        "print(generated_words2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BPE (Byte Pair Encoding) токенизация\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Установка библиотеки tokenizers для BPE\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "try:\n",
        "    from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
        "    print(\"Библиотека tokenizers уже установлена\")\n",
        "except ImportError:\n",
        "    print(\"Устанавливаем библиотеку tokenizers...\")\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"tokenizers\"])\n",
        "    from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "print(\"Настраиваем BPE токенизацию...\")\n",
        "\n",
        "# Создаем и обучаем BPE токенизатор\n",
        "bpe_tokenizer = Tokenizer(models.BPE())\n",
        "trainer = trainers.BpeTrainer(special_tokens=[\"<PAD>\"])\n",
        "bpe_tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
        "bpe_tokenizer.train_from_iterator(full_cleaned_text.splitlines(), trainer)\n",
        "\n",
        "print(\"BPE токенизатор обучен!\")\n",
        "\n",
        "# Создание последовательностей с BPE токенизацией\n",
        "tokens = bpe_tokenizer.encode(full_cleaned_text).ids\n",
        "seq_len = 40\n",
        "X_bpe, y_bpe = [], []\n",
        "\n",
        "for i in range(seq_len, len(tokens)):\n",
        "    X_bpe.append(tokens[i-seq_len:i])\n",
        "    y_bpe.append(tokens[i])\n",
        "\n",
        "vocab_size_bpe = bpe_tokenizer.get_vocab_size()\n",
        "\n",
        "X_bpe = np.array(X_bpe)\n",
        "y_bpe = to_categorical(y_bpe, num_classes=vocab_size_bpe)\n",
        "\n",
        "print(f\"Размер словаря BPE: {vocab_size_bpe}\")\n",
        "print(f\"Размер обучающих данных: {X_bpe.shape}\")\n",
        "print(f\"Размер целевых данных: {y_bpe.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Однослойная LSTM с BPE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Создание однослойной LSTM модели с BPE\n",
        "model_bpe_1 = Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size_bpe, 64, input_length=seq_len),\n",
        "    LSTM(128),\n",
        "    Dense(vocab_size_bpe, activation='softmax')\n",
        "])\n",
        "\n",
        "model_bpe_1.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "print(\"Архитектура однослойной LSTM с BPE:\")\n",
        "model_bpe_1.summary()\n",
        "\n",
        "# Обучение модели\n",
        "print(\"Начинаем обучение однослойной LSTM с BPE...\")\n",
        "model_bpe_1.fit(X_bpe, y_bpe, epochs=150, batch_size=128)\n",
        "\n",
        "print(\"Обучение завершено!\")\n",
        "\n",
        "# Функция генерации с BPE\n",
        "def generate_text_bpe_lstm(model, tokenizer_bpe, seed_text, gen_tokens=50, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Функция генерации текста с использованием BPE токенизации\n",
        "    \n",
        "    Args:\n",
        "        model: Обученная модель\n",
        "        tokenizer_bpe: BPE токенизатор\n",
        "        seed_text (str): Начальный текст\n",
        "        gen_tokens (int): Количество токенов для генерации\n",
        "        temperature (float): Температура сэмплирования\n",
        "        \n",
        "    Returns:\n",
        "        str: Сгенерированный текст\n",
        "    \"\"\"\n",
        "    tokens = tokenizer_bpe.encode(seed_text).ids\n",
        "    generated = tokens[:]\n",
        "\n",
        "    for _ in range(gen_tokens):\n",
        "        input_seq = generated[-seq_len:]\n",
        "        input_seq = pad_sequences([input_seq], maxlen=seq_len)\n",
        "\n",
        "        preds = model.predict(input_seq, verbose=0)[0]\n",
        "        preds = np.log(preds + 1e-8) / temperature\n",
        "        exp_preds = np.exp(preds)\n",
        "        preds = exp_preds / np.sum(exp_preds)\n",
        "\n",
        "        next_index = np.random.choice(range(vocab_size_bpe), p=preds)\n",
        "        generated.append(next_index)\n",
        "\n",
        "    decoded = tokenizer_bpe.decode(generated)\n",
        "    return decoded\n",
        "\n",
        "# Тестируем генерацию с BPE\n",
        "print(\"Генерация текста с BPE токенизацией:\")\n",
        "print(\"Начальная фраза: 'заклинание против'\")\n",
        "generated_bpe = generate_text_bpe_lstm(model_bpe_1, bpe_tokenizer, \"заклинание против\")\n",
        "print(generated_bpe)\n",
        "\n",
        "print(\"\\nГенерация с другой фразой:\")\n",
        "print(\"Начальная фраза: 'проклятые маглы'\")\n",
        "generated_bpe2 = generate_text_bpe_lstm(model_bpe_1, bpe_tokenizer, \"проклятые маглы\")\n",
        "print(generated_bpe2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Часть 3: Двунаправленная LSTM\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Посимвольная токенизация с Bidirectional LSTM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Импорт библиотек для Bidirectional LSTM\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "print(\"Настраиваем Bidirectional LSTM для посимвольной генерации...\")\n",
        "\n",
        "# Подготовка данных (аналогично предыдущим моделям)\n",
        "chars = sorted(set(full_cleaned_text))\n",
        "char2idx = {c: i for i, c in enumerate(chars)}\n",
        "idx2char = {i: c for c, i in char2idx.items()}\n",
        "vocab_size = len(chars)\n",
        "\n",
        "# Создание последовательностей\n",
        "seq_length = 40\n",
        "step = 3\n",
        "input_seqs, target_chars = [], []\n",
        "\n",
        "for i in range(0, len(full_cleaned_text) - seq_length, step):\n",
        "    seq = full_cleaned_text[i:i + seq_length]\n",
        "    target = full_cleaned_text[i + seq_length]\n",
        "    input_seqs.append([char2idx[c] for c in seq])\n",
        "    target_chars.append(char2idx[target])\n",
        "\n",
        "X_char = tf.keras.utils.to_categorical(input_seqs, num_classes=vocab_size)\n",
        "y_char = tf.keras.utils.to_categorical(target_chars, num_classes=vocab_size)\n",
        "\n",
        "print(f\"Размер обучающих данных: {X_char.shape}\")\n",
        "print(f\"Размер целевых данных: {y_char.shape}\")\n",
        "\n",
        "# Создание модели Bidirectional LSTM\n",
        "model_char = Sequential([\n",
        "    Bidirectional(LSTM(128), input_shape=(seq_length, vocab_size)),\n",
        "    Dense(vocab_size, activation='softmax')\n",
        "])\n",
        "\n",
        "model_char.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "print(\"Архитектура Bidirectional LSTM:\")\n",
        "model_char.summary()\n",
        "\n",
        "# Обучение модели\n",
        "print(\"Начинаем обучение Bidirectional LSTM...\")\n",
        "model_char.fit(X_char, y_char, batch_size=128, epochs=50)\n",
        "\n",
        "print(\"Обучение завершено!\")\n",
        "\n",
        "# Функция генерации для Bidirectional LSTM\n",
        "def generate_char_text(model, seed_text=\"гарри\", length=20, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Функция генерации текста с Bidirectional LSTM\n",
        "    \n",
        "    Args:\n",
        "        model: Обученная модель\n",
        "        seed_text (str): Начальный текст\n",
        "        length (int): Длина генерируемого текста\n",
        "        temperature (float): Температура сэмплирования\n",
        "        \n",
        "    Returns:\n",
        "        str: Сгенерированный текст\n",
        "    \"\"\"\n",
        "    generated = seed_text\n",
        "    \n",
        "    for _ in range(length):\n",
        "        input_seq = [char2idx.get(c, 0) for c in generated[-seq_length:]]\n",
        "        input_seq = tf.keras.utils.to_categorical(input_seq, num_classes=vocab_size)\n",
        "        input_seq = np.expand_dims(input_seq, axis=0)\n",
        "\n",
        "        preds = model.predict(input_seq, verbose=0)[0]\n",
        "        preds = np.log(preds + 1e-8) / temperature\n",
        "        preds = np.exp(preds) / np.sum(np.exp(preds))\n",
        "\n",
        "        next_idx = np.random.choice(range(vocab_size), p=preds)\n",
        "        next_char = idx2char[next_idx]\n",
        "        generated += next_char\n",
        "    \n",
        "    return generated\n",
        "\n",
        "# Тестируем генерацию\n",
        "print(\"Генерация текста с Bidirectional LSTM:\")\n",
        "print(\"Начальное слово: 'заклинание'\")\n",
        "generated_char = generate_char_text(model_char, \"заклинание\", length=60)\n",
        "print(generated_char)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Часть 4: GPT с нуля\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Символьная GPT модель\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Импорт библиотек для Transformer архитектуры\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "\n",
        "print(\"Создаем GPT-style модель с нуля...\")\n",
        "\n",
        "# Подготовка данных для символов\n",
        "vocab = sorted(set(full_cleaned_text))\n",
        "char2idx = {c: i for i, c in enumerate(vocab)}\n",
        "idx2char = {i: c for c, i in char2idx.items()}\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "print(f\"Размер словаря символов: {vocab_size}\")\n",
        "\n",
        "# Преобразование в числовой вид\n",
        "encoded = [char2idx[c] for c in full_cleaned_text]\n",
        "\n",
        "# Создание обучающих пар\n",
        "context_len = 64\n",
        "X, y = [], []\n",
        "for i in range(len(encoded) - context_len):\n",
        "    X.append(encoded[i:i + context_len])\n",
        "    y.append(encoded[i + context_len])\n",
        "\n",
        "X = np.array(X)\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "\n",
        "print(f\"Размер входных данных: {X.shape}\")\n",
        "print(f\"Размер целевых данных: {y.shape}\")\n",
        "\n",
        "# Определение кастомных слоев для Transformer\n",
        "from tensorflow.keras.layers import Layer, Dense, LayerNormalization\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "class MaskedSelfAttention(Layer):\n",
        "    \"\"\"\n",
        "    Слой маскированного self-attention для GPT архитектуры\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.proj_q = Dense(embed_dim)\n",
        "        self.proj_k = Dense(embed_dim)\n",
        "        self.proj_v = Dense(embed_dim)\n",
        "        self.out = Dense(embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        B, T, C = tf.shape(x)[0], tf.shape(x)[1], self.embed_dim\n",
        "        q = self.proj_q(x)\n",
        "        k = self.proj_k(x)\n",
        "        v = self.proj_v(x)\n",
        "\n",
        "        # Разделение на головы внимания\n",
        "        q = tf.concat(tf.split(q, self.num_heads, axis=-1), axis=0)\n",
        "        k = tf.concat(tf.split(k, self.num_heads, axis=-1), axis=0)\n",
        "        v = tf.concat(tf.split(v, self.num_heads, axis=-1), axis=0)\n",
        "\n",
        "        # Вычисление attention scores\n",
        "        att = tf.matmul(q, k, transpose_b=True) / tf.math.sqrt(tf.cast(C // self.num_heads, tf.float32))\n",
        "\n",
        "        # Маскирование будущих позиций (causal mask)\n",
        "        mask = tf.linalg.band_part(tf.ones((T, T)), -1, 0)\n",
        "        att = tf.where(mask == 0, -1e10, att)\n",
        "\n",
        "        att = tf.nn.softmax(att, axis=-1)\n",
        "        out = tf.matmul(att, v)\n",
        "        out = tf.concat(tf.split(out, self.num_heads, axis=0), axis=-1)\n",
        "        return self.out(out)\n",
        "\n",
        "class TransformerBlock(Layer):\n",
        "    \"\"\"\n",
        "    Блок Transformer с self-attention и feed-forward сетью\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim):\n",
        "        super().__init__()\n",
        "        self.att = MaskedSelfAttention(embed_dim, num_heads)\n",
        "        self.norm1 = LayerNormalization()\n",
        "        self.ff = tf.keras.Sequential([\n",
        "            Dense(ff_dim, activation='relu'),\n",
        "            Dense(embed_dim)\n",
        "        ])\n",
        "        self.norm2 = LayerNormalization()\n",
        "\n",
        "    def call(self, x):\n",
        "        # Self-attention с residual connection\n",
        "        x = x + self.att(self.norm1(x))\n",
        "        # Feed-forward с residual connection\n",
        "        x = x + self.ff(self.norm2(x))\n",
        "        return x\n",
        "\n",
        "class PositionalEncoding(Layer):\n",
        "    \"\"\"\n",
        "    Позиционное кодирование для Transformer\n",
        "    \"\"\"\n",
        "    def __init__(self, max_len, embed_dim):\n",
        "        super().__init__()\n",
        "        pos = np.arange(max_len)[:, None]\n",
        "        i = np.arange(embed_dim)[None, :]\n",
        "        angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(embed_dim))\n",
        "        angle_rads = pos * angle_rates\n",
        "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "        self.pos_encoding = tf.constant(angle_rads[np.newaxis, ...], dtype=tf.float32)\n",
        "\n",
        "    def call(self, x):\n",
        "        return x + self.pos_encoding[:, :tf.shape(x)[1], :]\n",
        "\n",
        "print(\"Кастомные слои Transformer определены!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Создание GPT модели\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding\n",
        "\n",
        "# Параметры модели\n",
        "embed_dim = 128\n",
        "num_heads = 4\n",
        "ff_dim = 256\n",
        "num_layers = 4\n",
        "\n",
        "print(f\"Создаем GPT модель с параметрами:\")\n",
        "print(f\"- Размерность эмбеддинга: {embed_dim}\")\n",
        "print(f\"- Количество голов внимания: {num_heads}\")\n",
        "print(f\"- Размерность feed-forward: {ff_dim}\")\n",
        "print(f\"- Количество слоев: {num_layers}\")\n",
        "\n",
        "# Создание модели\n",
        "inp = Input(shape=(context_len,))\n",
        "x = Embedding(vocab_size, embed_dim)(inp)\n",
        "x = PositionalEncoding(context_len, embed_dim)(x)\n",
        "\n",
        "# Добавляем блоки Transformer\n",
        "for i in range(num_layers):\n",
        "    x = TransformerBlock(embed_dim, num_heads, ff_dim)(x)\n",
        "\n",
        "# Выходной слой для предсказания следующего токена\n",
        "out = Dense(vocab_size, activation=\"softmax\")(x[:, -1, :])\n",
        "\n",
        "gpt_model = Model(inputs=inp, outputs=out)\n",
        "gpt_model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")\n",
        "\n",
        "print(\"Архитектура GPT модели:\")\n",
        "gpt_model.summary()\n",
        "\n",
        "# Обучение модели\n",
        "print(\"Начинаем обучение GPT модели...\")\n",
        "print(\"Внимание: Обучение может занять значительное время!\")\n",
        "\n",
        "gpt_model.fit(X, y, batch_size=128, epochs=10)\n",
        "\n",
        "print(\"Обучение GPT модели завершено!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_gpt(seed_text, length=300, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Функция генерации текста с помощью GPT модели\n",
        "    \n",
        "    Args:\n",
        "        seed_text (str): Начальный текст\n",
        "        length (int): Длина генерируемого текста\n",
        "        temperature (float): Температура сэмплирования\n",
        "        \n",
        "    Returns:\n",
        "        str: Сгенерированный текст\n",
        "    \"\"\"\n",
        "    generated = seed_text\n",
        "    context = [char2idx.get(c, 0) for c in seed_text][-context_len:]\n",
        "\n",
        "    for _ in range(length):\n",
        "        # Подготавливаем входную последовательность\n",
        "        padded = pad_sequences([context], maxlen=context_len)\n",
        "        preds = gpt_model.predict(padded, verbose=0)[0]\n",
        "\n",
        "        # Безопасное масштабирование вероятностей\n",
        "        preds = np.asarray(preds).astype(\"float64\")\n",
        "        preds = np.log(np.clip(preds, 1e-8, 1.0)) / temperature\n",
        "        preds = np.exp(preds)\n",
        "        preds = preds / np.sum(preds)\n",
        "\n",
        "        # Выбираем следующий символ\n",
        "        next_idx = np.random.choice(len(preds), p=preds)\n",
        "        next_char = idx2char[next_idx]\n",
        "\n",
        "        generated += next_char\n",
        "        context.append(next_idx)\n",
        "        context = context[-context_len:]  # Сохраняем только последние context_len символов\n",
        "\n",
        "    return generated\n",
        "\n",
        "# Тестируем генерацию с GPT\n",
        "print(\"Генерация текста с GPT моделью:\")\n",
        "print(\"Начальное слово: 'гарри'\")\n",
        "generated_gpt = generate_gpt(\"гарри \", 300, temperature=1.0)\n",
        "print(generated_gpt)\n",
        "\n",
        "print(\"\\nГенерация с другим начальным словом:\")\n",
        "print(\"Начальное слово: 'гарри' (короткая генерация)\")\n",
        "generated_gpt_short = generate_gpt(\"гарри \", 40)\n",
        "print(generated_gpt_short)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Дообучение готовой GPT модели\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Установка библиотек для дообучения\n",
        "try:\n",
        "    from datasets import Dataset\n",
        "    from transformers import GPT2Tokenizer, GPT2LMHeadModel, TrainingArguments, Trainer\n",
        "    print(\"Библиотеки transformers и datasets уже установлены\")\n",
        "except ImportError:\n",
        "    print(\"Устанавливаем необходимые библиотеки...\")\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"transformers\", \"datasets\"])\n",
        "    from datasets import Dataset\n",
        "    from transformers import GPT2Tokenizer, GPT2LMHeadModel, TrainingArguments, Trainer\n",
        "\n",
        "print(\"Настраиваем дообучение готовой GPT модели...\")\n",
        "\n",
        "# Создание датасета из наших данных\n",
        "dataset = Dataset.from_pandas(df[[\"text\"]])\n",
        "\n",
        "print(f\"Создан датасет с {len(dataset)} записями\")\n",
        "\n",
        "# Загрузка предобученного токенизатора для русского языка\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"sberbank-ai/rugpt3small_based_on_gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def tokenize_fn(batch):\n",
        "    \"\"\"\n",
        "    Функция токенизации для датасета\n",
        "    \"\"\"\n",
        "    encodings = tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
        "    encodings[\"labels\"] = encodings[\"input_ids\"].copy()\n",
        "    return encodings\n",
        "\n",
        "# Токенизация датасета\n",
        "tokenized = dataset.map(tokenize_fn, batched=True)\n",
        "\n",
        "print(\"Датасет токенизирован!\")\n",
        "\n",
        "# Загрузка предобученной модели\n",
        "model = GPT2LMHeadModel.from_pretrained(\"sberbank-ai/rugpt3small_based_on_gpt2\")\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "print(\"Предобученная модель загружена!\")\n",
        "print(f\"Размер модели: {model.num_parameters():,} параметров\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Настройка параметров обучения\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./rugpt-finetuned\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=2,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    logging_steps=50,\n",
        "    prediction_loss_only=True,\n",
        ")\n",
        "\n",
        "# Создание тренера\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "print(\"Начинаем дообучение модели...\")\n",
        "print(\"Это может занять значительное время!\")\n",
        "\n",
        "# Запуск обучения\n",
        "trainer.train()\n",
        "\n",
        "print(\"Дообучение завершено!\")\n",
        "\n",
        "# Создание pipeline для генерации\n",
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "print(\"Pipeline для генерации создан!\")\n",
        "\n",
        "# Тестируем генерацию с дообученной моделью\n",
        "print(\"Генерация текста с дообученной моделью:\")\n",
        "print(\"Начальная фраза: 'в хогвартсе'\")\n",
        "generated_finetuned = generator(\"в хогвартсе\", max_length=100, do_sample=True, temperature=0.9)\n",
        "print(generated_finetuned[0]['generated_text'])\n",
        "\n",
        "print(\"\\nГенерация с другой фразой:\")\n",
        "print(\"Начальная фраза: 'гермиона сказала'\")\n",
        "generated_finetuned2 = generator(\"гермиона сказала\", max_length=100, do_sample=True, temperature=1.0)\n",
        "print(generated_finetuned2[0]['generated_text'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Заключение и сравнение моделей\n",
        "\n",
        "## Анализ результатов генерации\n",
        "\n",
        "В данной лабораторной работе мы реализовали и протестировали различные подходы к генерации текста:\n",
        "\n",
        "### 1. Simple RNN\n",
        "- **Преимущества**: Простота реализации, быстрая тренировка\n",
        "- **Недостатки**: Проблема исчезающего градиента, плохое качество генерации\n",
        "- **Применение**: Базовый уровень для понимания принципов работы\n",
        "\n",
        "### 2. LSTM (однонаправленная и многослойная)\n",
        "- **Преимущества**: Решение проблемы долгосрочных зависимостей, лучшее качество генерации\n",
        "- **Недостатки**: Медленная тренировка, ограниченная контекстная память\n",
        "- **Применение**: Хорошо подходит для последовательностей средней длины\n",
        "\n",
        "### 3. Bidirectional LSTM\n",
        "- **Преимущества**: Двунаправленная обработка контекста\n",
        "- **Недостатки**: Не подходит для генерации в реальном времени\n",
        "- **Применение**: Анализ и понимание текста, но не генерация\n",
        "\n",
        "### 4. GPT-style Transformer\n",
        "- **Преимущества**: Параллельная обработка, механизм внимания, высокое качество\n",
        "- **Недостатки**: Требует много вычислительных ресурсов\n",
        "- **Применение**: Современный стандарт для генерации текста\n",
        "\n",
        "### 5. Дообучение готовых моделей\n",
        "- **Преимущества**: Использование предобученных знаний, быстрое дообучение\n",
        "- **Недостатки**: Зависимость от качества предобученной модели\n",
        "- **Применение**: Практический подход для реальных задач\n",
        "\n",
        "## Рекомендации по выбору архитектуры\n",
        "\n",
        "- **Для изучения**: Начните с Simple RNN, затем переходите к LSTM\n",
        "- **Для экспериментов**: Используйте LSTM с различными токенизациями\n",
        "- **Для производства**: Рассмотрите Transformer архитектуры или дообучение готовых моделей\n",
        "- **Для русского языка**: Предпочтительно использовать предобученные модели с дообучением\n",
        "\n",
        "## Дальнейшее развитие\n",
        "\n",
        "1. **Увеличение размера модели**: Больше параметров = лучшее качество\n",
        "2. **Улучшение токенизации**: BPE, SentencePiece, современные методы\n",
        "3. **Оптимизация обучения**: Learning rate scheduling, gradient clipping\n",
        "4. **Оценка качества**: BLEU, ROUGE, человеческая оценка\n",
        "5. **Специализация**: Модели для конкретных доменов или стилей\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
