# Лабораторная работа №1: Нейросетевой фреймворк

```
╔══════════════════════════════════════════════════════════════════════════════╗
║                                                                              ║
║  ФИО: Крижановская Анна Александровна                                        ║
║  Группа: М80-109Св-24 (М8О-109Св-24)                                         ║
║  Курс: Нейронные сети и глубокое обучение                                    ║
║  Лабораторная работа №1                                                      ║
║                                                                              ║
╚══════════════════════════════════════════════════════════════════════════════╝
```

## Задание

**Цель:** Реализовать собственный нейросетевой фреймворк для обучения полносвязных нейронных сетей.

**Требования:**
1. **Архитектура сети** - описание нейросетей как набора слоёв с разными функциями активации
2. **Оптимизаторы** - разные алгоритмы оптимизации (SGD, Momentum SGD, RMSProp) - минимум 2
3. **Датасеты** - несколько готовых датасетов (MNIST, FashionMNIST, Iris) - минимум 2
4. **Обработка данных** - функции для работы с датасетами (батчинг, перемешивание, map)
5. **Примеры** - реализовать примеры регрессии и классификации на данном фреймворке

**Реализованные элементы:**
-  3 оптимизатора (SGD, Momentum, RMSProp)
-  3 датасета (MNIST, FashionMNIST, Iris)
-  5 функций активации (ReLU, Sigmoid, Tanh, Softmax, Threshold)
-  4 функции потерь (CrossEntropy, BinaryCrossEntropy, MSE, MAE)
-  Функции обработки данных (батчинг, перемешивание, map)
-  Примеры классификации и регрессии

---

# Нейросетевой фреймворк для машинного обучения

## Описание

Собственный нейросетевой фреймворк для обучения полносвязных нейронных сетей с расширенными возможностями для решения задач классификации и регрессии. Фреймворк реализован с нуля на Python с использованием NumPy и предоставляет простой и гибкий интерфейс для создания и обучения нейронных сетей.

## Основные возможности

###  Архитектура сети
- Создание многослойных нейросетей с перечислением слоёв
- Поддержка различных типов слоёв (полносвязные, активационные)
- Гибкая настройка архитектуры сети

###  Обработка данных
- Удобные функции для работы с датасетами
- Поддержка minibatching и перемешивания данных
- Автоматическая загрузка популярных датасетов (MNIST, Iris, FashionMNIST)

###  Оптимизаторы
- **StochasticGradientDescent** - классический стохастический градиентный спуск
- **MomentumOptimizer** - оптимизатор с моментом для ускорения сходимости
- **RMSPropOptimizer** - адаптивный оптимизатор с экспоненциальным затуханием

###  Функции активации
- **ReLUActivation** - Rectified Linear Unit
- **SigmoidActivation** - сигмоидальная функция
- **TanhActivation** - гиперболический тангенс
- **SoftmaxActivation** - softmax для многоклассовой классификации
- **ThresholdActivation** - пороговая функция активации

###  Функции потерь
- **CrossEntropyLoss** - перекрестная энтропия для классификации
- **BinaryCrossEntropyLoss** - бинарная перекрестная энтропия
- **MeanSquaredErrorLoss** - среднеквадратичная ошибка для регрессии
- **MeanAbsoluteErrorLoss** - средняя абсолютная ошибка

###  Поддерживаемые задачи
- Классификация изображений (MNIST, FashionMNIST)
- Классификация табличных данных (Iris)
- Бинарная классификация
- Регрессионный анализ


### Простейший пример классификации

```python
from framework import (
    DeepLearningFramework, DataLoader, DenseLayer, 
    ReLUActivation, SoftmaxActivation, CrossEntropyLoss,
    StochasticGradientDescent
)

# Загрузка данных
data_loader = DataLoader("Iris")
X_train, y_train = data_loader.get_training_data()
X_test, y_test = data_loader.get_test_data()

# Создание модели
model = DeepLearningFramework()
model.build_network(
    layers=[
        DenseLayer(3, 7), 
        ReLUActivation(), 
        DenseLayer(7, 3), 
        SoftmaxActivation()
    ],
    loss_function=CrossEntropyLoss(),
    optimizer=StochasticGradientDescent()
)

# Обучение
model.train_model(X_train, y_train)

# Предсказание
predictions, accuracy = model.make_predictions(X_test, y_test)
print(f'Точность: {accuracy}')
```

## Детальное описание компонентов

### Основные классы

#### `DeepLearningFramework`
Главный класс фреймворка для создания и обучения нейронных сетей.

**Основные методы:**
- `build_network(layers, loss_function, optimizer, learning_rate, epochs_count)` - настройка сети
- `train_model(training_features, training_labels, batch_size)` - обучение модели
- `make_predictions(test_features, test_labels)` - предсказание на тестовых данных

#### `NeuralNetwork`
Класс нейронной сети, объединяющий слои.

**Методы:**
- `forward_pass(input_data)` - прямой проход
- `backward_pass(gradient)` - обратный проход
- `update_parameters(learning_rate, optimizer)` - обновление параметров

### Слои нейронной сети

#### `DenseLayer(input_size, output_size)`
Полносвязный слой нейронной сети.

**Параметры:**
- `input_size` - размер входного вектора
- `output_size` - количество нейронов в слое

#### Функции активации

```python
# ReLU активация
relu = ReLUActivation()

# Сигмоидальная активация
sigmoid = SigmoidActivation()

# Гиперболический тангенс
tanh = TanhActivation()

# Softmax для многоклассовой классификации
softmax = SoftmaxActivation()

# Пороговая активация
threshold = ThresholdActivation()
```

### Оптимизаторы

#### `StochasticGradientDescent`
Классический стохастический градиентный спуск.

```python
optimizer = StochasticGradientDescent()
```

#### `MomentumOptimizer(momentum=0.9)`
Оптимизатор с моментом для ускорения сходимости.

```python
optimizer = MomentumOptimizer(momentum=0.9)
```

#### `RMSPropOptimizer(rho=0.9, epsilon=1e-8)`
Адаптивный оптимизатор с экспоненциальным затуханием.

```python
optimizer = RMSPropOptimizer(rho=0.9, epsilon=1e-8)
```

### Функции потерь

#### Для классификации

```python
# Перекрестная энтропия для многоклассовой классификации
loss = CrossEntropyLoss()

# Бинарная перекрестная энтропия
loss = BinaryCrossEntropyLoss()
```

#### Для регрессии

```python
# Среднеквадратичная ошибка
loss = MeanSquaredErrorLoss()

# Средняя абсолютная ошибка
loss = MeanAbsoluteErrorLoss()
```

### Загрузка данных

#### `DataLoader(dataset_name, test_size=0.2, random_state=42)`
Класс для загрузки популярных датасетов.

**Поддерживаемые датасеты:**
- `"MNIST"` - рукописные цифры
- `"FashionMNIST"` - изображения одежды
- `"Iris"` - классификация ирисов

```python
# Загрузка MNIST
data_loader = DataLoader("MNIST")
X_train, y_train = data_loader.get_training_data(num_samples=1000)
X_test, y_test = data_loader.get_test_data(num_samples=500)
```

## Примеры использования

### 1. Классификация изображений MNIST

```python
from framework import *

# Загрузка данных
data_loader = DataLoader("MNIST")
X_train, y_train = data_loader.get_training_data(num_samples=1000)
X_test, y_test = data_loader.get_test_data(num_samples=500)

# Создание модели
model = DeepLearningFramework()
model.build_network(
    layers=[
        DenseLayer(784, 1960), 
        ReLUActivation(), 
        DenseLayer(1960, 1176), 
        TanhActivation(), 
        DenseLayer(1176, 10), 
        SoftmaxActivation()
    ],
    loss_function=CrossEntropyLoss(), 
    epochs_count=7
)

# Обучение и предсказание
model.train_model(X_train, y_train)
predictions, accuracy = model.make_predictions(X_test, y_test)
print(f'Точность MNIST: {accuracy}')
```

### 2. Регрессионный анализ

```python
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split

# Генерация данных
X, y = make_regression(n_samples=400, n_features=1, noise=10, random_state=0)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=100, random_state=42)

# Создание модели
model = DeepLearningFramework()
model.build_network(
    layers=[DenseLayer(1, 1)],
    loss_function=MeanSquaredErrorLoss(),
    optimizer=StochasticGradientDescent(), 
    regression=True
)

# Обучение
model.train_model(X_train, y_train)
predictions, loss = model.make_predictions(X_test, y_test)
print(f'Потери регрессии: {loss}')
```

### 3. Бинарная классификация

```python
from sklearn.datasets import make_classification

# Генерация данных
X, y = make_classification(n_samples=1000, n_features=2, n_classes=2, random_state=0)
X_train, X_test = np.split(X, [800])
y_train, y_test = np.split(y, [800])

# Создание модели
model = DeepLearningFramework()
model.build_network(
    layers=[
        DenseLayer(2, 5), 
        TanhActivation(), 
        DenseLayer(5, 2), 
        SoftmaxActivation()
    ], 
    loss_function=CrossEntropyLoss()
)

# Обучение
model.train_model(X_train, y_train)
predictions, accuracy = model.make_predictions(X_test, y_test)
print(f'Точность: {accuracy}')
```

## Сравнение оптимизаторов

Фреймворк поддерживает несколько оптимизаторов для различных задач:

| Оптимизатор | Скорость сходимости | Стабильность | Рекомендуется для |
|-------------|-------------------|--------------|-------------------|
| SGD | Средняя | Средняя | Простые задачи |
| Momentum | Высокая | Высокая | Большие датасеты |
| RMSProp | Высокая | Очень высокая | Нестабильные градиенты |

## Архитектурные решения

### Модульность
Фреймворк построен по модульному принципу, где каждый компонент может быть заменён независимо:
- Слои нейронной сети
- Функции активации
- Оптимизаторы
- Функции потерь

### Расширяемость
Легко добавлять новые компоненты, наследуясь от базовых классов:
- `BaseOptimizer` для новых оптимизаторов
- Базовые классы для слоёв и функций активации

### Производительность
- Векторизованные операции с NumPy
- Эффективная реализация обратного распространения
- Поддержка батчевой обработки

## Результаты

MNIST
Обучение модели:
Эпоха 1: точность = 0.861
Эпоха 2: точность = 0.903
Эпоха 3: точность = 0.934
Эпоха 4: точность = 0.944
Эпоха 5: точность = 0.963
Эпоха 6: точность = 0.973
Эпоха 7: точность = 0.978
Точность классификации MNIST: 0.866

Iris
Обучение модели:
Эпоха 1: точность = 0.325
Эпоха 2: точность = 0.325
Эпоха 3: точность = 0.325
Эпоха 4: точность = 0.325
Эпоха 5: точность = 0.325
Эпоха 6: точность = 0.6916666666666667
Эпоха 7: точность = 0.6916666666666667
Эпоха 8: точность = 0.6916666666666667
Эпоха 9: точность = 0.6916666666666667
Эпоха 10: точность = 0.6916666666666667
Эпоха 11: точность = 0.6916666666666667
Эпоха 12: точность = 0.6916666666666667
Эпоха 13: точность = 0.6916666666666667
Эпоха 14: точность = 0.6916666666666667
Эпоха 15: точность = 0.6916666666666667
Эпоха 16: точность = 0.6916666666666667
Эпоха 17: точность = 0.6916666666666667
Эпоха 18: точность = 0.6916666666666667
Эпоха 19: точность = 0.6916666666666667
Эпоха 20: точность = 0.6916666666666667
Точность классификации Iris: 0.5666666666666667

Бинарная классификация
Обучение модели:
Эпоха 1: точность = 0.84875
Эпоха 2: точность = 0.85375
Эпоха 3: точность = 0.86
Эпоха 4: точность = 0.8625
Эпоха 5: точность = 0.86375
Эпоха 6: точность = 0.86625
Эпоха 7: точность = 0.865
Эпоха 8: точность = 0.8625
Эпоха 9: точность = 0.86625
Эпоха 10: точность = 0.86625
Эпоха 11: точность = 0.865
Эпоха 12: точность = 0.86625
Эпоха 13: точность = 0.865
Эпоха 14: точность = 0.86625
Эпоха 15: точность = 0.865
Эпоха 16: точность = 0.865
Эпоха 17: точность = 0.865
Эпоха 18: точность = 0.86625
Эпоха 19: точность = 0.87
Эпоха 20: точность = 0.86625
Точность бинарной классификации: 0.85

Регрессионный анализ
Обучение модели:
Эпоха 1: потери = 42.594648111984284
Эпоха 2: потери = 83.1241090893494
Эпоха 3: потери = 118.75005648672851
Эпоха 4: потери = 110.81017494790785
Эпоха 5: потери = 135.86760500666122
Эпоха 6: потери = 81.60343462166415
Эпоха 7: потери = 105.35506548736782
Эпоха 8: потери = 125.08117013729759
Эпоха 9: потери = 114.11228701687354
Эпоха 10: потери = 137.69308821063487
Эпоха 11: потери = 104.4303761726357
Эпоха 12: потери = 82.17274223024197
Эпоха 13: потери = 77.1338328772596
Эпоха 14: потери = 98.50503385756235
Эпоха 15: потери = 55.007519079313916
Эпоха 16: потери = 90.01501370256464
Эпоха 17: потери = 54.86331970195778
Эпоха 18: потери = 85.551868092374
Эпоха 19: потери = 203.42499663509932
Эпоха 20: потери = 126.92949205415317
Потери регрессии: 104.56296779997982


## Сравнение оптимизаторов

| Оптимизатор | MNIST | Iris | Бинарная | Регрессия |
|-------------|-------|------|----------|-----------|
| SGD | 86.6% | 56.7% | 85.0% | 104.56 |
| Momentum | - | - | - | 111.32 |
| RMSProp | - | - | - | - |


## Заключение

### Достигнутые результаты

1. **Успешно реализован нейросетевой фреймворк** с модульной архитектурой
2. **Поддержка различных задач:**
   - Классификация изображений (MNIST)
   - Классификация табличных данных (Iris)
   - Бинарная классификация
   - Регрессионный анализ

3. **Реализованы оптимизаторы:**
   - StochasticGradientDescent (используется в 4 примерах)
   - MomentumOptimizer (используется в 1 примере)
   - RMSPropOptimizer (только импортируется, не используется)

4. **Функции активации:**
   - ReLU, Sigmoid, Tanh, Softmax, Threshold

5. **Функции потерь:**
   - CrossEntropyLoss, BinaryCrossEntropyLoss, MeanSquaredErrorLoss, MeanAbsoluteErrorLoss

### Преимущества фреймворка

- **Простота использования** - обучение модели в несколько строк кода
- **Гибкость** - легко настраиваемые параметры и архитектура
- **Модульность** - каждый компонент может быть заменён независимо
- **Читаемость** - понятные названия методов и классов
- **Расширяемость** - легко добавлять новые слои, оптимизаторы и функции потерь

---
